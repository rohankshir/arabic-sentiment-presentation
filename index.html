<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>reveal.js</title>

		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/black.css">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
			  <section>Open Domain Targeted Sentiment Analysis in Arabic
			    <p class="fragment fade-up">Using Neural Networks</p>
			  </section>

			  <!-- Background -->
			  <section>
			    <section>Background</section>
			    <section>
			      <p>Open Domain Targeted SA investigates the classification of sentiment polarities towards certain target entity mentions. </p>
			      <p>Targets aren't given to you (as opposed to Targeted Sentiment Analysis)</p>			      
			      <p>Targets are not limited to Named Entities. Can be any Noun Phrase </p>
			    </section>
			    <section>
			      <h3>Examples of targets</h3>
			      <p>singing in the bath</p>
			      <p>the best defense against Trump</p>
			      <p>Windows 7</p>
			      <p>American imperialism</p>
			    </section>
			    <section>
			      <p>Uses in typical NLP tasks such as Information Extraction, Summarization, Question-Answering</p>
			      <p>Source help in a crisis</p>
			      <p class="fragment highlight-blue" >Understand public opinion</p>
			    </section>
			    <section>
			      <img  src="http://images.says.com/uploads/story_source/source_image/361632/9c5c.jpg">
			    </section>
			    <section>
			      <h4 class="fragment fade-up">NLP can help</h4>
			      <img  src="https://pdgc2015a.files.wordpress.com/2016/03/arabspring.jpg?w=1000&h=600">

			    </section>
		
			    
			    <section>
			      <p>
				هذه من منجزات (الثورة) و الأتي أعظم،، هنيئاً لكم ثورتكم،،و بعد التجارب الليبية و العراقية و المصرية و ما شاهدناه من دمار و قتل و إستعمار،، نحن في سوريا لا نريد ثورة الفوضة الخلاقة الأميركية. نريد سوريا الأمن و الأمان و العروبة الصادقة</p>
			      <p class="fragment fade-up">What're the targets of positive and negative sentiment here?</p>
			    </section>
			    <section>
			      Translation provided by Google
			      <p>This is one of the achievements of the <b class="fragment highlight-red">revolution</b> and more to come, Congratulations to you and your revolution, after <b class="fragment highlight-red">Libyan experiences and Iraqi and Egyptian</b> and what we saw destruction and killing and colonization, We in Syria do not want the American creative chaotic revolution. We want <b class="fragment highlight-green"> Syria's security and safety of Arabism and sincere </b>
			      </p>
			    </section>
			  </section>


			  <!-- Noura's work -->			  
			  <section>
			    <section> Noura's work </section>
			    <section> We use the Arabic Opinion Target dataset developed by Noura
			      <p>TODO cite</p>
			    </section>
			    <section>
			      <p>1200 online comments posted to Aljazeera newspaper articles</p>
			      <p> 4886 targets: 38.2% positive, 50.5% negative, and 11.3% ambiguous</p>
			      <p> ~ 100,000 words and a vocabulary ~ 10,000</p>
			    </section>
			    <section data-background-color="white">
			      <div  style="height: 600px; width: 1000px;" id="5d8bdbc3-7a74-45fc-a61a-83408ac56397"  class="plotly-graph-div"></div>
			    </section>
			    <section data-background-color="white">
			      Sequence Labeling task
			      <img style="" src="figure2.png"  >
			    </section>
			    <section>
			      <h4>Morphology</h4>
			      <p><b>ATB</b> splitting off all the clitics and particles, but keeps the definite article Al+ attached  </p>
			      <p><b>D3</b> splitting off all the clitics, particles, and prepositions (Al+)  </p>
			      <p>Original:"bEwn AlHkwmh" </p><p> Translation(lemma): with(help)from the(Government)</p>
			      <p class="fragment">ATB Tokenized: b+, Eawon_1, Hukuwmap_1</p>
			      <p class="fragment">D3 Tokenized:b+, Eawon_1, Al+, Hukuwmap_1</p>
			    </section>
			    <section> 
			      <p>Pipelined two CRF models </p>
			      <p>Tested with different morphological schemes using MADAMIRA </p>
			      <p>Introduced rich linguistic features<p>
			      <p>TODO cite</p>
			    </section>
			    <section>
			      <p>We continued this approach of breaking the task down into two components</p>
			      <p>Allows for modularity</p>
			      <p>Can better address sparsity issues on a small dataset</p>
			      <p>Easier to "debug"</p>
			    </section>
			    <section>
			      <h4> Two tasks</h4>
			      <p>Target Extraction (as a sequence labeling task)</p>
			      <img border="0" src="arrow.png">
			      <p>Target Dependent Sentiment Classification (as a binary classification task)</p>
			    </section>
			  </section>

			  <section>
			    <section>Neural Target Extraction</section>
			    <section>
			      <p>Can we beat the CRF results?</p>
			      <p>Can we greatly reduce the feature engineering step?
			      </p>
			    </section>
			    <section  data-background-color="white"
				      data-background-size="800px"
				      data-background-position="50% 50%"
				      data-background-image="http://i.stack.imgur.com/kwhAP.jpg">
			      <h4 style="position:fixed; top:200px;left:400px;">TODO cite</h4>
			    </section>
			    <section  data-background-color="white"
				      data-background-size="120px"
				      data-background-position="50% 10%"
				      data-background-image="https://lh3.googleusercontent.com/-WsOKUehwbZQ/VZr3XE4PeKI/AAAAAAAACYw/cea1qP4J3iM/s1600/many-to-many.PNG">
			      <p>We predict targets using a Many to Many Architecture</p>
			    </section>
			    <section>
			      RNN/LSTMs are a helpful abstraction to represent sequences in Neural Networks
			      <p>TODO cite</p>
			    </section>

			    
			    <section>
			      <h3>Word Vectors</h3>
			      <p>Used Arabic Wikidump to pretrain word vectors</p>
			      <p>Used fasttext library to train skipgram with negative sampling with dimension 50</p>
			      <p>Trained word vectors on a variety of morphological schemes (lemma, ATB, D3)<p>
			    </section>
			    <section data-background-color="white">
			      <p>Initial architecture we want to build</p>
			      <img src="single_rnn.png">
			    </section>
			    <section>
			      <h3>Keras</h3>
			      <p>Python Deep Learning library
			      <p>Easy to experiment with different architectures</p>
			      <p>Tensorflow or Theano Backend <p>
			    </section>
			    <section>
			      <p>Word features</p>

			      <pre><code data-trim data-noescape>
lexical_model = Sequential()
lexical_model.add(Embedding(
    server.vocab_size,
    embedding_size,
    input_shape=MAX_SEQ_LEN,
<mark>    weights=embedding_weights,</mark>
    trainable=args.fine_tune))

			      </code></pre>
			    </section>
			    <section>
			      <p>POS features</p>

			      <pre><code data-trim data-noescape>
pos_model = Sequential()
pos_model.add(Embedding(
    <mark>num_pos_tags,</mark>
    input_shape=MAX_SEQ_LEN,
    init="glorot_normal",
    trainable=args.fine_tune))

			      </code></pre>
			    </section>			    

			    
			    <section>
			      <p>Merge</p>

			      <pre><code data-trim data-noescape>
model_list = <mark >[lexical_model, pos_model]</mark> 
merged =Merge(model_list, mode='concat')
model = Sequential()
model.add(<mark>merged</mark>)
model.add(
    LSTM(RNN_SIZE,
         return_sequences=True,
         dropout_W=.2))
model.add(Dropout(0.2))
			      </code></pre>
			    </section>

			    <section>
			      <p>Prediction and Compilation</p>

			      <pre><code data-trim data-noescape>
model.add(TimeDistributed(Dense(server.num_classes, activation='softmax')))               
model.compile(loss='categorical_crossentropy',                                   
              <mark>optimizer='rmsprop',</mark>
              sample_weight_mode='temporal',
              metrics=['accuracy'])
			      </code></pre>
			    </section>
			  </section>
			  <section>
			    <section>
			      Error Analysis
			    </section>
			    <section>
			      <p>
				No knowledge of Arabic</p>
			      <p>But I do know English :)</p>
			    </section>
			    <section>
			      <p>Figure out simple way to understand errors on validation data</p>
			      <img src="error_analysis_lex.png">

			    </section>
			    <section>
			      <p>Key insight</p>
			      <img src="error_analysis.png">
			      <p class="fragment fade-down">Process input in reverse<p>
			    </section>
			     <section data-background-color="white">
			      <h4>Final Architecture</h4>
			      <img src="multi_rnn.png">
			    </section>
			  </section>
			  <section>
			    <section> Hyperparameter Experiments</section>
			    <section> Dropout, Unknown Words, Epochs</section>
			    <section data-background-color="white">
			      <div  style="height: 600px; width: 1000px;" id="aeec9e92-f68d-459a-af78-68ccc0998b30"  class="plotly-graph-div"></div>
			      
			    </section>
			    <section> D3 vs ATB </section>
			    <section data-background-color="white">
			      <div  style="height: 600px; width: 1000px;" id="9336a8f3-b15a-40d7-bb9a-967ef9e12852"  class="plotly-graph-div"></div>
			    </section>
			    <section> Cost sensitive classification </section>
			    <section data-background-color="white">
			      <div  style="height: 600px; width: 1000px;" id="4d91f07d-b69b-4b8b-a8e6-29d28f71b492"  class="plotly-graph-div"></div>
			    </section>
			    <section> More experiments </section>
			    <section> <h4>Retrofitting</h4>
			      <p>Dataset was too small for any real benefit</p>
			    </section>
			    <section> Transfer Learning
			      <pre><code data-trim data-noescape>
model.load_weights(args.transfer_model, by_name=True)
			      </code></pre>
			    </section>
			    <section>
			      <p>Collapsed Model</p>
			      <p>Data sparsity issues</p>
			    </section>
			  </section>
			  <section>
			    <section>Results</section>
			    <section data-markdown>
			      <script type="text/template">
| Features         | CRF Precision | CRF Recall | CRF F1 | LSTM Precision | LSTM Recall | LSTM F1 |
|------------------|---------------|------------|--------|----------------|-------------|---------|
| Surface + POS    | 41.0          | 60.6       | 48.9   | 43.3           | 64.7        | **54.8**    |
| Lemma + POS      | 48.2          | 60.5       | 53.7   | 54.0           | 72.8        | **62.0**    |
| + D3             | 59.6          | 55.7       | 57.6   | 43.1           | 89.0        | **58.0**    |
| + ATB            | 52.4          | 59.5       | 55.7   | 53.0           | 73.9        | **61.7**    |
| Best Linguistic  | 66.2          | 57.8       | 61.8   |                |             |         |
			      </script>
			    </section>
			    <section>
			      <img src="figure_1.png"  >
			    </section>
			    <section>
			      Sentiment Classification work is ongoing..
			    </section>
			  </section>
			  <section>
			    <h4> Key takeaways</h4>
			    <p>With less data, break the task down</p>
			    <p>Take full advantage of latent representions with less sparsity </p>
			    <p>Do error analysis</p>
			    <p>Word vectors + RNNs = Powerful building blocks to play around with in <i>any</i> language </p>
			  </section>
			</div>
		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>
		<script src="js/plotly-latest.min.js"></script>

		<script>
			// More info https://github.com/hakimel/reveal.js#configuration
			Reveal.initialize({
				history: true,

				// More info https://github.com/hakimel/reveal.js#dependencies
				dependencies: [
					{ src: 'plugin/markdown/marked.js' },
					{ src: 'plugin/markdown/markdown.js' },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } }
				]
			});
		</script>
				<script type="text/javascript">window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL="https://plot.ly";Plotly.newPlot("aeec9e92-f68d-459a-af78-68ccc0998b30", [{"name": "lemma_ent_results", "text": ["15e2unk.0.1dropout_pos.0.0dropout_lex.ents", "15e2unk.0.4dropout_pos.0.2dropout_lex.ents", "15e2unk.0.3dropout_pos.0.2dropout_lex.ents", "15e2unk.0.4dropout_pos.0.3dropout_lex.ents", "15e2unk.0.1dropout_pos.0.1dropout_lex.ents", "15e2unk.0.3dropout_pos.0.4dropout_lex.ents", "15e2unk.0.3dropout_pos.0.3dropout_lex.ents", "15e2unk.0.2dropout_pos.0.0dropout_lex.ents", "15e2unk.0.2dropout_pos.0.2dropout_lex.ents", "15e2unk.0.2dropout_pos.0.3dropout_lex.ents", "15e2unk.0.2dropout_pos.0.4dropout_lex.ents", "15e2unk.0.3dropout_pos.0.1dropout_lex.ents", "15e2unk.0.4dropout_pos.0.4dropout_lex.ents", "15e2unk.0.1dropout_pos.0.3dropout_lex.ents", "15e2unk.0.4dropout_pos.0.1dropout_lex.ents", "15e2unk.0.1dropout_pos.0.2dropout_lex.ents", "15e2unk.0.1dropout_pos.0.4dropout_lex.ents", "15e2unk.0.2dropout_pos.0.1dropout_lex.ents", "15e2unk.0.4dropout_pos.0.0dropout_lex.ents", "15e2unk.0.3dropout_pos.0.0dropout_lex.ents", "15e2unk.0.0dropout_pos.0.2dropout_lex.ents", "15e2unk.0.0dropout_pos.0.3dropout_lex.ents", "15e2unk.0.0dropout_pos.0.0dropout_lex.ents", "15e2unk.0.0dropout_pos.0.4dropout_lex.ents", "15e2unk.0.0dropout_pos.0.1dropout_lex.ents"], "mode": "markers", "y": [0.55497382199, 0.551020408163, 0.542429284526, 0.555369127517, 0.553719008264, 0.559602649007, 0.550653594771, 0.54028436019, 0.537267080745, 0.534562211982, 0.542056074766, 0.532012195122, 0.549450549451, 0.529411764706, 0.538109756098, 0.519650655022, 0.533733133433, 0.52700729927, 0.536404160475, 0.529753265602, 0.512, 0.502564102564, 0.5, 0.494464944649, 0.493268053856], "x": [0.616780045351, 0.625850340136, 0.628117913832, 0.628117913832, 0.630385487528, 0.639455782313, 0.641723356009, 0.650793650794, 0.657596371882, 0.659863945578, 0.659863945578, 0.659863945578, 0.664399092971, 0.668934240363, 0.668934240363, 0.678004535147, 0.680272108844, 0.687074829932, 0.687074829932, 0.698412698413, 0.732426303855, 0.746031746032, 0.748299319728, 0.764172335601, 0.766439909297], "type": "scatter"}, {"name": "epoch_and_unk_experiments", "text": ["skipgram.epoch12.unk1.0.2dropout", "skipgram.epoch12.unk6.0.2dropout", "skipgram.epoch30.unk5.0.2dropout", "skipgram.epoch10.unk6.0.2dropout", "skipgram.epoch15.unk5.0.2dropout", "skipgram.epoch15.unk6.0.2dropout", "skipgram.epoch20.unk6.0.2dropout", "skipgram.epoch12.unk5.0.2dropout", "skipgram.epoch20.unk5.0.2dropout", "skipgram.epoch30.unk6.0.2dropout", "skipgram.epoch12.unk3.0.2dropout", "skipgram.epoch30.unk4.0.2dropout", "skipgram.epoch15.unk3.0.2dropout", "skipgram.epoch12.unk4.0.2dropout", "skipgram.epoch20.unk3.0.2dropout", "skipgram.epoch10.unk5.0.2dropout", "skipgram.epoch15.unk4.0.2dropout", "skipgram.epoch15.unk1.0.2dropout", "skipgram.epoch20.unk4.0.2dropout", "skipgram.epoch30.unk1.0.2dropout", "skipgram.epoch10.unk1.0.2dropout", "skipgram.epoch10.unk3.0.2dropout", "skipgram.epoch20.unk1.0.2dropout", "skipgram.epoch30.unk3.0.2dropout", "skipgram.epoch10.unk2.0.2dropout", "skipgram.epoch30.unk2.0.2dropout", "skipgram.epoch12.unk2.0.2dropout", "skipgram.epoch10.unk4.0.2dropout", "skipgram.epoch20.unk2.0.2dropout", "skipgram.epoch15.unk2.0.2dropout"], "mode": "markers", "y": [0.600451467269, 0.581052631579, 0.573964497041, 0.565217391304, 0.569811320755, 0.566990291262, 0.560377358491, 0.572232645403, 0.56102003643, 0.552293577982, 0.570652173913, 0.537704918033, 0.564924114671, 0.538704581359, 0.539556962025, 0.540752351097, 0.541139240506, 0.565074135091, 0.518624641834, 0.547112462006, 0.532188841202, 0.52849002849, 0.553191489362, 0.52915451895, 0.540462427746, 0.51904090268, 0.523874488404, 0.496259351621, 0.493365500603, 0.497048406139], "x": [0.510204081633, 0.52380952381, 0.541950113379, 0.557823129252, 0.562358276644, 0.562358276644, 0.569160997732, 0.573696145125, 0.580498866213, 0.580498866213, 0.607709750567, 0.614512471655, 0.634920634921, 0.641723356009, 0.648526077098, 0.650793650794, 0.650793650794, 0.657596371882, 0.68253968254, 0.689342403628, 0.698412698413, 0.698412698413, 0.698412698413, 0.698412698413, 0.702947845805, 0.71201814059, 0.727891156463, 0.748299319728, 0.770975056689, 0.791383219955], "type": "scatter"}], {"title": "Precision vs Recall", "xaxis": {"title": "Recall"}, "yaxis": {"title": "Precision"}}, {"linkText": "Export to plot.ly", "showLink": false})</script><script type="text/javascript">window.removeEventListener("resize");window.addEventListener("resize", function(){Plotly.Plots.resize(document.getElementById("aeec9e92-f68d-459a-af78-68ccc0998b30"));});
		</script>
		<script>
		  window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL="https://plot.ly";Plotly.newPlot("5d8bdbc3-7a74-45fc-a61a-83408ac56397", [{"y": [9.592741540485795, 8.416488487294606, 7.903596289614301, 7.503289630675082, 7.488852955733459, 7.45298232946546, 7.423568444259167, 7.31455283232408, 7.069023426578259, 6.926577033222725, 6.915723448631314, 6.892641641172089, 6.890609120147166, 6.890609120147166, 6.517671272912275, 6.507277712385012, 6.336825731146441, 6.304448802421981, 6.261491684321042, 6.248042874508429, 6.240275845170769, 6.12029741895095, 6.0844994130751715, 6.066108090103747, 5.976350909297934, 5.823045895483019, 5.768320995793772, 5.765191102784844, 5.752572638825633, 5.749392985908253, 5.717027701406222, 5.68697535633982, 5.645446897643238, 5.545177444479562, 5.529429087511423, 5.5254529391317835, 5.501258210544727, 5.420534999272286, 5.3981627015177525, 5.37989735354046, 5.308267697401205, 5.247024072160486, 5.231108616854587, 5.225746673713202, 5.214935757608986, 5.204006687076795, 5.170483995038151, 5.170483995038151, 5.1647859739235145, 5.1647859739235145, 5.147494476813453, 5.123963979403259, 5.056245805348308, 5.049856007249537, 5.0106352940962555, 5.003946305945459, 4.969813299576001, 4.927253685157205, 4.919980925828125, 4.912654885736052, 4.90527477843843, 4.897839799950911, 4.897839799950911, 4.804021044733257, 4.804021044733257, 4.77912349311153, 4.770684624465665, 4.727387818712341, 4.709530201312334, 4.6913478822291435, 4.6913478822291435, 4.672828834461906, 4.672828834461906, 4.663439094112067, 4.6443908991413725, 4.634728988229636, 4.634728988229636, 4.61512051684126, 4.59511985013459, 4.59511985013459, 4.574710978503383, 4.564348191467836, 4.564348191467836, 4.553876891600541, 4.553876891600541, 4.543294782270004, 4.543294782270004, 4.543294782270004, 4.532599493153256, 4.499809670330265, 4.477336814478207, 4.477336814478207, 4.465908118654584, 4.454347296253507, 4.454347296253507, 4.454347296253507, 4.442651256490317, 4.430816798843313, 4.418840607796598, 4.418840607796598, 4.406719247264253, 4.406719247264253, 4.3694478524670215, 4.3694478524670215, 4.3694478524670215, 4.356708826689592, 4.356708826689592, 4.356708826689592, 4.356708826689592, 4.343805421853684, 4.330733340286331, 4.330733340286331, 4.31748811353631, 4.31748811353631, 4.30406509320417, 4.30406509320417, 4.30406509320417, 4.290459441148391, 4.276666119016055, 4.2626798770413155, 4.248495242049359, 4.248495242049359, 4.23410650459726, 4.219507705176107, 4.219507705176107, 4.204692619390966, 4.204692619390966, 4.204692619390966, 4.204692619390966, 4.189654742026425, 4.1588830833596715, 4.1588830833596715, 4.143134726391533, 4.143134726391533, 4.143134726391533, 4.143134726391533, 4.143134726391533, 4.127134385045092, 4.127134385045092, 4.127134385045092, 4.127134385045092, 4.127134385045092, 4.127134385045092, 4.110873864173311, 4.0943445622221, 4.0943445622221, 4.0943445622221, 4.07753744390572, 4.07753744390572, 4.07753744390572, 4.07753744390572, 4.060443010546419, 4.060443010546419, 4.04305126783455, 4.04305126783455, 4.04305126783455, 4.04305126783455, 4.02535169073515, 4.007333185232471, 4.007333185232471, 3.9889840465642745, 3.9889840465642745, 3.970291913552122, 3.970291913552122, 3.970291913552122, 3.970291913552122, 3.970291913552122, 3.9512437185814275, 3.9512437185814275, 3.9512437185814275, 3.9512437185814275, 3.9318256327243257, 3.9318256327243257, 3.9318256327243257, 3.9318256327243257, 3.9318256327243257, 3.912023005428146, 3.912023005428146, 3.912023005428146, 3.8918202981106265, 3.8918202981106265, 3.8918202981106265, 3.8918202981106265, 3.871201010907891, 3.871201010907891, 3.871201010907891, 3.8501476017100584, 3.8501476017100584, 3.8501476017100584, 3.828641396489095, 3.828641396489095, 3.828641396489095, 3.828641396489095, 3.8066624897703196, 3.8066624897703196, 3.8066624897703196, 3.8066624897703196, 3.8066624897703196, 3.8066624897703196, 3.8066624897703196, 3.8066624897703196, 3.784189633918261, 3.784189633918261, 3.784189633918261, 3.784189633918261, 3.7612001156935624, 3.7612001156935624, 3.7612001156935624, 3.7376696182833684, 3.7376696182833684, 3.7376696182833684, 3.7376696182833684, 3.7376696182833684, 3.7376696182833684, 3.7376696182833684, 3.7376696182833684, 3.7376696182833684, 3.713572066704308, 3.713572066704308, 3.713572066704308, 3.713572066704308, 3.713572066704308, 3.713572066704308, 3.713572066704308, 3.6888794541139363, 3.6888794541139363, 3.6888794541139363, 3.6888794541139363, 3.6888794541139363, 3.6888794541139363, 3.6888794541139363, 3.6888794541139363, 3.6888794541139363, 3.6888794541139363, 3.6635616461296463, 3.6635616461296463, 3.6635616461296463, 3.6635616461296463, 3.6635616461296463, 3.6635616461296463, 3.6635616461296463, 3.6635616461296463, 3.6635616461296463, 3.6375861597263857, 3.6375861597263857, 3.6375861597263857, 3.6375861597263857, 3.6375861597263857, 3.6375861597263857, 3.6109179126442243, 3.6109179126442243, 3.6109179126442243, 3.6109179126442243, 3.6109179126442243, 3.6109179126442243, 3.6109179126442243, 3.6109179126442243, 3.6109179126442243, 3.6109179126442243, 3.6109179126442243, 3.58351893845611, 3.58351893845611, 3.58351893845611, 3.58351893845611, 3.58351893845611, 3.58351893845611, 3.58351893845611, 3.58351893845611, 3.58351893845611, 3.58351893845611, 3.58351893845611, 3.5553480614894135, 3.5553480614894135, 3.5553480614894135, 3.5553480614894135, 3.5553480614894135, 3.5553480614894135, 3.5553480614894135, 3.5553480614894135, 3.5553480614894135, 3.5553480614894135, 3.5553480614894135, 3.5553480614894135, 3.5553480614894135, 3.5553480614894135, 3.5263605246161616, 3.5263605246161616, 3.5263605246161616, 3.5263605246161616, 3.5263605246161616, 3.5263605246161616, 3.5263605246161616, 3.5263605246161616, 3.5263605246161616, 3.5263605246161616, 3.4965075614664802, 3.4965075614664802, 3.4965075614664802, 3.4965075614664802, 3.4965075614664802, 3.4965075614664802, 3.4965075614664802, 3.4965075614664802, 3.4965075614664802, 3.4657359027997265, 3.4657359027997265, 3.4657359027997265, 3.4657359027997265, 3.4657359027997265, 3.4657359027997265, 3.4657359027997265, 3.4657359027997265, 3.4657359027997265, 3.4657359027997265, 3.4657359027997265, 3.4339872044851463, 3.4339872044851463, 3.4339872044851463, 3.4339872044851463, 3.4339872044851463, 3.4339872044851463, 3.4339872044851463, 3.4011973816621555, 3.4011973816621555, 3.4011973816621555, 3.4011973816621555, 3.4011973816621555, 3.4011973816621555, 3.4011973816621555, 3.4011973816621555, 3.4011973816621555, 3.4011973816621555, 3.4011973816621555, 3.4011973816621555, 3.367295829986474, 3.367295829986474, 3.367295829986474, 3.367295829986474, 3.367295829986474, 3.367295829986474, 3.367295829986474, 3.367295829986474, 3.367295829986474, 3.367295829986474, 3.367295829986474, 3.367295829986474, 3.367295829986474, 3.367295829986474, 3.367295829986474, 3.367295829986474, 3.332204510175204, 3.332204510175204, 3.332204510175204, 3.332204510175204, 3.332204510175204, 3.332204510175204, 3.332204510175204, 3.332204510175204, 3.332204510175204, 3.332204510175204, 3.332204510175204, 3.332204510175204, 3.332204510175204, 3.332204510175204, 3.295836866004329, 3.295836866004329, 3.295836866004329, 3.295836866004329, 3.295836866004329, 3.295836866004329, 3.295836866004329, 3.295836866004329, 3.295836866004329, 3.295836866004329, 3.258096538021482, 3.258096538021482, 3.258096538021482, 3.258096538021482, 3.258096538021482, 3.258096538021482, 3.258096538021482, 3.258096538021482, 3.258096538021482, 3.258096538021482, 3.258096538021482, 3.258096538021482, 3.258096538021482, 3.258096538021482, 3.258096538021482, 3.2188758248682006, 3.2188758248682006, 3.2188758248682006, 3.2188758248682006, 3.2188758248682006, 3.2188758248682006, 3.2188758248682006, 3.2188758248682006, 3.2188758248682006, 3.2188758248682006, 3.2188758248682006, 3.2188758248682006, 3.2188758248682006, 3.1780538303479458, 3.1780538303479458, 3.1780538303479458, 3.1780538303479458, 3.1780538303479458, 3.1780538303479458, 3.1780538303479458, 3.1780538303479458, 3.1780538303479458, 3.1780538303479458, 3.1780538303479458, 3.1780538303479458, 3.1780538303479458, 3.1780538303479458, 3.1780538303479458, 3.1780538303479458, 3.1354942159291497, 3.1354942159291497, 3.1354942159291497, 3.1354942159291497, 3.1354942159291497, 3.1354942159291497, 3.1354942159291497, 3.1354942159291497, 3.1354942159291497, 3.1354942159291497, 3.1354942159291497, 3.1354942159291497, 3.1354942159291497, 3.1354942159291497, 3.1354942159291497, 3.1354942159291497, 3.1354942159291497, 3.1354942159291497, 3.1354942159291497, 3.1354942159291497, 3.1354942159291497, 3.1354942159291497, 3.091042453358316, 3.091042453358316, 3.091042453358316, 3.091042453358316, 3.091042453358316, 3.091042453358316, 3.091042453358316, 3.091042453358316, 3.091042453358316, 3.091042453358316, 3.091042453358316, 3.091042453358316, 3.091042453358316, 3.091042453358316, 3.091042453358316, 3.091042453358316, 3.091042453358316, 3.091042453358316, 3.091042453358316, 3.091042453358316, 3.091042453358316, 3.091042453358316, 3.091042453358316, 3.091042453358316, 3.091042453358316, 3.091042453358316, 3.044522437723423, 3.044522437723423, 3.044522437723423, 3.044522437723423, 3.044522437723423, 3.044522437723423, 3.044522437723423, 3.044522437723423, 3.044522437723423, 3.044522437723423, 3.044522437723423, 3.044522437723423, 3.044522437723423, 3.044522437723423, 3.044522437723423, 3.044522437723423, 3.044522437723423, 3.044522437723423, 3.044522437723423, 3.044522437723423, 3.044522437723423, 3.044522437723423, 3.044522437723423, 2.995732273553991, 2.995732273553991, 2.995732273553991, 2.995732273553991, 2.995732273553991, 2.995732273553991, 2.995732273553991, 2.995732273553991, 2.995732273553991, 2.995732273553991, 2.995732273553991, 2.995732273553991, 2.995732273553991, 2.995732273553991, 2.995732273553991, 2.995732273553991, 2.995732273553991, 2.995732273553991, 2.995732273553991, 2.995732273553991, 2.995732273553991, 2.995732273553991, 2.995732273553991, 2.995732273553991, 2.995732273553991, 2.995732273553991, 2.995732273553991, 2.995732273553991, 2.995732273553991, 2.995732273553991, 2.995732273553991, 2.995732273553991, 2.995732273553991, 2.9444389791664403, 2.9444389791664403, 2.9444389791664403, 2.9444389791664403, 2.9444389791664403, 2.9444389791664403, 2.9444389791664403, 2.9444389791664403, 2.9444389791664403, 2.9444389791664403, 2.9444389791664403, 2.9444389791664403, 2.9444389791664403, 2.9444389791664403, 2.9444389791664403, 2.9444389791664403, 2.9444389791664403, 2.9444389791664403, 2.9444389791664403, 2.9444389791664403, 2.9444389791664403, 2.9444389791664403, 2.9444389791664403, 2.9444389791664403, 2.9444389791664403, 2.9444389791664403, 2.9444389791664403, 2.8903717578961645, 2.8903717578961645, 2.8903717578961645, 2.8903717578961645, 2.8903717578961645, 2.8903717578961645, 2.8903717578961645, 2.8903717578961645, 2.8903717578961645, 2.8903717578961645, 2.8903717578961645, 2.8903717578961645, 2.8903717578961645, 2.8903717578961645, 2.8903717578961645, 2.8903717578961645, 2.8903717578961645, 2.8903717578961645, 2.8903717578961645, 2.8903717578961645, 2.8903717578961645, 2.8903717578961645, 2.8903717578961645, 2.8903717578961645, 2.8903717578961645, 2.8903717578961645, 2.8903717578961645, 2.8903717578961645, 2.8903717578961645, 2.8903717578961645, 2.833213344056216, 2.833213344056216, 2.833213344056216, 2.833213344056216, 2.833213344056216, 2.833213344056216, 2.833213344056216, 2.833213344056216, 2.833213344056216, 2.833213344056216, 2.833213344056216, 2.833213344056216, 2.833213344056216, 2.833213344056216, 2.833213344056216, 2.833213344056216, 2.833213344056216, 2.833213344056216, 2.833213344056216, 2.833213344056216, 2.833213344056216, 2.833213344056216, 2.833213344056216, 2.833213344056216, 2.833213344056216, 2.833213344056216, 2.833213344056216, 2.833213344056216, 2.833213344056216, 2.772588722239781, 2.772588722239781, 2.772588722239781, 2.772588722239781, 2.772588722239781, 2.772588722239781, 2.772588722239781, 2.772588722239781, 2.772588722239781, 2.772588722239781, 2.772588722239781, 2.772588722239781, 2.772588722239781, 2.772588722239781, 2.772588722239781, 2.772588722239781, 2.772588722239781, 2.772588722239781, 2.772588722239781, 2.772588722239781, 2.772588722239781, 2.772588722239781, 2.772588722239781, 2.772588722239781, 2.772588722239781, 2.772588722239781, 2.772588722239781, 2.772588722239781, 2.772588722239781, 2.772588722239781, 2.772588722239781, 2.772588722239781, 2.772588722239781, 2.772588722239781, 2.772588722239781, 2.772588722239781, 2.772588722239781, 2.772588722239781, 2.772588722239781, 2.772588722239781, 2.772588722239781, 2.772588722239781, 2.772588722239781, 2.772588722239781, 2.772588722239781, 2.772588722239781, 2.772588722239781, 2.772588722239781, 2.772588722239781, 2.772588722239781, 2.772588722239781, 2.70805020110221, 2.70805020110221, 2.70805020110221, 2.70805020110221, 2.70805020110221, 2.70805020110221, 2.70805020110221, 2.70805020110221, 2.70805020110221, 2.70805020110221, 2.70805020110221, 2.70805020110221, 2.70805020110221, 2.70805020110221, 2.70805020110221, 2.70805020110221, 2.70805020110221, 2.70805020110221, 2.70805020110221, 2.70805020110221, 2.70805020110221, 2.70805020110221, 2.70805020110221, 2.70805020110221, 2.70805020110221, 2.70805020110221, 2.70805020110221, 2.70805020110221, 2.70805020110221, 2.70805020110221, 2.70805020110221, 2.70805020110221, 2.70805020110221, 2.70805020110221, 2.70805020110221, 2.70805020110221, 2.6390573296152584, 2.6390573296152584, 2.6390573296152584, 2.6390573296152584, 2.6390573296152584, 2.6390573296152584, 2.6390573296152584, 2.6390573296152584, 2.6390573296152584, 2.6390573296152584, 2.6390573296152584, 2.6390573296152584, 2.6390573296152584, 2.6390573296152584, 2.6390573296152584, 2.6390573296152584, 2.6390573296152584, 2.6390573296152584, 2.6390573296152584, 2.6390573296152584, 2.6390573296152584, 2.6390573296152584, 2.6390573296152584, 2.6390573296152584, 2.6390573296152584, 2.6390573296152584, 2.6390573296152584, 2.6390573296152584, 2.6390573296152584, 2.6390573296152584, 2.6390573296152584, 2.6390573296152584, 2.6390573296152584, 2.6390573296152584, 2.6390573296152584, 2.6390573296152584, 2.6390573296152584, 2.6390573296152584, 2.6390573296152584, 2.6390573296152584, 2.6390573296152584, 2.6390573296152584, 2.6390573296152584, 2.6390573296152584, 2.6390573296152584, 2.6390573296152584, 2.6390573296152584, 2.6390573296152584, 2.6390573296152584, 2.6390573296152584, 2.6390573296152584, 2.6390573296152584, 2.6390573296152584, 2.6390573296152584, 2.6390573296152584, 2.6390573296152584, 2.6390573296152584, 2.6390573296152584, 2.6390573296152584, 2.6390573296152584, 2.6390573296152584, 2.5649493574615367, 2.5649493574615367, 2.5649493574615367, 2.5649493574615367, 2.5649493574615367, 2.5649493574615367, 2.5649493574615367, 2.5649493574615367, 2.5649493574615367, 2.5649493574615367, 2.5649493574615367, 2.5649493574615367, 2.5649493574615367, 2.5649493574615367, 2.5649493574615367, 2.5649493574615367, 2.5649493574615367, 2.5649493574615367, 2.5649493574615367, 2.5649493574615367, 2.5649493574615367, 2.5649493574615367, 2.5649493574615367, 2.5649493574615367, 2.5649493574615367, 2.5649493574615367, 2.5649493574615367, 2.5649493574615367, 2.5649493574615367, 2.5649493574615367, 2.5649493574615367, 2.5649493574615367, 2.5649493574615367, 2.5649493574615367, 2.5649493574615367, 2.5649493574615367, 2.5649493574615367, 2.5649493574615367, 2.5649493574615367, 2.5649493574615367, 2.5649493574615367, 2.5649493574615367, 2.5649493574615367, 2.4849066497880004, 2.4849066497880004, 2.4849066497880004, 2.4849066497880004, 2.4849066497880004, 2.4849066497880004, 2.4849066497880004, 2.4849066497880004, 2.4849066497880004, 2.4849066497880004, 2.4849066497880004, 2.4849066497880004, 2.4849066497880004, 2.4849066497880004, 2.4849066497880004, 2.4849066497880004, 2.4849066497880004, 2.4849066497880004, 2.4849066497880004, 2.4849066497880004, 2.4849066497880004, 2.4849066497880004, 2.4849066497880004, 2.4849066497880004, 2.4849066497880004, 2.4849066497880004, 2.4849066497880004, 2.4849066497880004, 2.4849066497880004, 2.4849066497880004, 2.4849066497880004, 2.4849066497880004, 2.4849066497880004, 2.4849066497880004, 2.4849066497880004, 2.4849066497880004, 2.4849066497880004, 2.4849066497880004, 2.4849066497880004, 2.4849066497880004, 2.4849066497880004, 2.4849066497880004, 2.4849066497880004, 2.4849066497880004, 2.4849066497880004, 2.4849066497880004, 2.4849066497880004, 2.4849066497880004, 2.4849066497880004, 2.4849066497880004, 2.4849066497880004, 2.4849066497880004, 2.3978952727983707, 2.3978952727983707, 2.3978952727983707, 2.3978952727983707, 2.3978952727983707, 2.3978952727983707, 2.3978952727983707, 2.3978952727983707, 2.3978952727983707, 2.3978952727983707, 2.3978952727983707, 2.3978952727983707, 2.3978952727983707, 2.3978952727983707, 2.3978952727983707, 2.3978952727983707, 2.3978952727983707, 2.3978952727983707, 2.3978952727983707, 2.3978952727983707, 2.3978952727983707, 2.3978952727983707, 2.3978952727983707, 2.3978952727983707, 2.3978952727983707, 2.3978952727983707, 2.3978952727983707, 2.3978952727983707, 2.3978952727983707, 2.3978952727983707, 2.3978952727983707, 2.3978952727983707, 2.3978952727983707, 2.3978952727983707, 2.3978952727983707, 2.3978952727983707, 2.3978952727983707, 2.3978952727983707, 2.3978952727983707, 2.3978952727983707, 2.3978952727983707, 2.3978952727983707, 2.3978952727983707, 2.3978952727983707, 2.3978952727983707, 2.3978952727983707, 2.3978952727983707, 2.3978952727983707, 2.3978952727983707, 2.3978952727983707, 2.3978952727983707, 2.3978952727983707, 2.3978952727983707, 2.3978952727983707, 2.3978952727983707, 2.3978952727983707, 2.3978952727983707, 2.3978952727983707, 2.3978952727983707, 2.3978952727983707, 2.3978952727983707, 2.3978952727983707, 2.3978952727983707, 2.3978952727983707, 2.3978952727983707, 2.3978952727983707, 2.3978952727983707, 2.3978952727983707, 2.3978952727983707, 2.3978952727983707, 2.3978952727983707, 2.3978952727983707, 2.3978952727983707, 2.3978952727983707, 2.3978952727983707, 2.3978952727983707, 2.3978952727983707, 2.302585092994046, 2.302585092994046, 2.302585092994046, 2.302585092994046, 2.302585092994046, 2.302585092994046, 2.302585092994046, 2.302585092994046, 2.302585092994046, 2.302585092994046, 2.302585092994046, 2.302585092994046, 2.302585092994046, 2.302585092994046, 2.302585092994046, 2.302585092994046, 2.302585092994046, 2.302585092994046, 2.302585092994046, 2.302585092994046, 2.302585092994046, 2.302585092994046, 2.302585092994046, 2.302585092994046, 2.302585092994046, 2.302585092994046, 2.302585092994046, 2.302585092994046, 2.302585092994046, 2.302585092994046, 2.302585092994046, 2.302585092994046, 2.302585092994046, 2.302585092994046, 2.302585092994046, 2.302585092994046, 2.302585092994046, 2.302585092994046, 2.302585092994046, 2.302585092994046, 2.302585092994046, 2.302585092994046, 2.302585092994046, 2.302585092994046, 2.302585092994046, 2.302585092994046, 2.302585092994046, 2.302585092994046, 2.302585092994046, 2.302585092994046, 2.302585092994046, 2.302585092994046, 2.302585092994046, 2.302585092994046, 2.302585092994046, 2.302585092994046, 2.302585092994046, 2.302585092994046, 2.302585092994046, 2.302585092994046, 2.302585092994046, 2.302585092994046, 2.302585092994046, 2.302585092994046, 2.302585092994046, 2.302585092994046, 2.302585092994046, 2.302585092994046, 2.302585092994046, 2.302585092994046, 2.302585092994046, 2.302585092994046, 2.302585092994046], "x": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1000, 1001], "type": "scatter", "mode": "markers"}], {"title": "Log Frequency vs Frequency Rank", "xaxis": {"title": "Frequency Rank"}, "yaxis": {"title": "Log Frequency"}}, {"linkText": "Export to plot.ly", "showLink": false})</script><script type="text/javascript">window.removeEventListener("resize");window.addEventListener("resize", function(){Plotly.Plots.resize(document.getElementById("5d8bdbc3-7a74-45fc-a61a-83408ac56397"));});

		</script>

		<script type="text/javascript">window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL="https://plot.ly";Plotly.newPlot("4d91f07d-b69b-4b8b-a8e6-29d28f71b492", [{"name": "cost_sensitive_target_experiments.txt", "text": ["multi.unk1.e20.pos.2lex.1.class_weights..25", "multi.unk1.e20.pos.2lex.1.class_weights..5", "multi.unk1.e20.pos.2lex.1.class_weights..6", "multi.unk1.e20.pos.2lex.1.class_weights..7", "multi.unk1.e20.pos.2lex.1.class_weights..8", "multi.unk1.e20.pos.2lex.1.class_weights..9", "multi.unk1.e20.pos.2lex.1.class_weights.1.1", "multi.unk1.e20.pos.2lex.1.class_weights.1", "multi.unk1.e20.pos.2lex.1.class_weights.2", "multi.unk1.e20.pos.2lex.1.class_weights.3", "multi.unk1.e20.pos.2lex.1.class_weights.4", "multi.unk1.e20.pos.2lex.1.class_weights.5"], "mode": "markers", "y": [0.209094419199, 0.479141422884, 0.519190648078, 0.548411416263, 0.576917079864, 0.605494880145, 0.613373146125, 0.61737722612, 0.545747841538, 0.53350619364, 0.532592335302, 0.526932306258], "x": [0.646464646465, 0.615384615385, 0.603092783505, 0.583877995643, 0.587301587302, 0.551505546751, 0.548192771084, 0.553191489362, 0.403353927626, 0.376335877863, 0.371747211896, 0.36523009496], "type": "scatter"}], {"title": "F1 vs Precision", "xaxis": {"title": "Precision"}, "yaxis": {"title": "F1"}}, {"linkText": "Export to plot.ly", "showLink": true})</script><script type="text/javascript">window.removeEventListener("resize");window.addEventListener("resize", function(){Plotly.Plots.resize(document.getElementById("4d91f07d-b69b-4b8b-a8e6-29d28f71b492"));});</script>

<script type="text/javascript">window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL="https://plot.ly";Plotly.newPlot("9336a8f3-b15a-40d7-bb9a-967ef9e12852", [{"name": "atb_results", "text": ["5e2unk.0.2dropout_pos.0.1dropout_lex", "10e2unk.0.2dropout_pos.0.1dropout_lex", "10e1unk.0.2dropout_pos.0.1dropout_lex", "30e1unk.0.2dropout_pos.0.1dropout_lex", "20e2unk.0.2dropout_pos.0.1dropout_lex", "15e2unk.0.2dropout_pos.0.1dropout_lex", "15e1unk.0.2dropout_pos.0.1dropout_lex", "20e1unk.0.2dropout_pos.0.1dropout_lex", "5e1unk.0.2dropout_pos.0.1dropout_lex"], "mode": "markers", "y": [0.527744846839, 0.568555367797, 0.590514800131, 0.592120139972, 0.599327187486, 0.598584279605, 0.604695480861, 0.617131296588, 0.5886640478], "x": [0.546534653465, 0.557093425606, 0.546875, 0.544740973312, 0.545864661654, 0.538686131387, 0.541364296081, 0.529649595687, 0.488066825776], "type": "scatter"}, {"name": "d3.dropout_experiments", "text": ["skipgram15e2unk.0.0dropout_pos.0..2dropout_lex", "skipgram15e2unk.0.0dropout_pos.0..1dropout_lex", "skipgram15e2unk.0.0dropout_pos.0..3dropout_lex", "skipgram15e2unk.0.0dropout_pos.0..5dropout_lex", "skipgram15e2unk.0.5dropout_pos.0..1dropout_lex", "skipgram15e2unk.0.3dropout_pos.0..0dropout_lex", "skipgram15e2unk.0.3dropout_pos.0..1dropout_lex", "skipgram15e2unk.0.4dropout_pos.0..1dropout_lex", "skipgram15e2unk.0.2dropout_pos.0..0dropout_lex", "skipgram15e2unk.0.1dropout_pos.0..0dropout_lex", "skipgram15e2unk.0.1dropout_pos.0..2dropout_lex", "skipgram15e2unk.0.0dropout_pos.0..4dropout_lex", "skipgram15e2unk.0.2dropout_pos.0..2dropout_lex", "skipgram15e2unk.0.5dropout_pos.0..0dropout_lex", "skipgram15e2unk.0.0dropout_pos.0..0dropout_lex", "skipgram15e2unk.0.5dropout_pos.0..2dropout_lex", "skipgram15e2unk.0.4dropout_pos.0..2dropout_lex", "skipgram15e2unk.0.5dropout_pos.0..3dropout_lex", "skipgram15e2unk.0.1dropout_pos.0..1dropout_lex", "skipgram15e2unk.0.4dropout_pos.0..4dropout_lex", "skipgram15e2unk.0.5dropout_pos.0..4dropout_lex", "skipgram15e2unk.0.2dropout_pos.0..1dropout_lex", "skipgram15e2unk.0.3dropout_pos.0..2dropout_lex", "skipgram15e2unk.0.4dropout_pos.0..3dropout_lex", "skipgram15e2unk.0.1dropout_pos.0..5dropout_lex", "skipgram15e2unk.0.3dropout_pos.0..5dropout_lex", "skipgram15e2unk.0.4dropout_pos.0..5dropout_lex", "skipgram15e2unk.0.5dropout_pos.0..5dropout_lex", "skipgram15e2unk.0.3dropout_pos.0..4dropout_lex", "skipgram15e2unk.0.4dropout_pos.0..0dropout_lex", "skipgram15e2unk.0.1dropout_pos.0..3dropout_lex", "skipgram15e2unk.0.2dropout_pos.0..4dropout_lex", "skipgram15e2unk.0.1dropout_pos.0..4dropout_lex", "skipgram15e2unk.0.2dropout_pos.0..3dropout_lex", "skipgram15e2unk.0.3dropout_pos.0..3dropout_lex"], "mode": "markers", "y": [0.557198862921, 0.560105847713, 0.552251486831, 0.555248017957, 0.561338949128, 0.574249542161, 0.568035943517, 0.568515738298, 0.570224349124, 0.571262764327, 0.563959923581, 0.565866521891, 0.565790360886, 0.559454702071, 0.56510383372, 0.571524012172, 0.577677653283, 0.584432775628, 0.574582293793, 0.574487835921, 0.582191219864, 0.577792084368, 0.578825949875, 0.580102887021, 0.578283702347, 0.578453843115, 0.578186279855, 0.584671220044, 0.578005631116, 0.57755410862, 0.579896907216, 0.575419223743, 0.577132804229, 0.578011186153, 0.581687336261], "x": [0.421736158578, 0.424470266576, 0.414937759336, 0.416038382454, 0.417859577369, 0.431752873563, 0.424766018719, 0.424746743849, 0.425, 0.425608011445, 0.417551309271, 0.419117647059, 0.419034090909, 0.410119840213, 0.415212840195, 0.420145791915, 0.425295343989, 0.432658757851, 0.420470262794, 0.42036910458, 0.428669882839, 0.423423423423, 0.424534803584, 0.425414364641, 0.422972972973, 0.423155044008, 0.422868741543, 0.42984257358, 0.422192151556, 0.421710526316, 0.423728813559, 0.418964357767, 0.42030934768, 0.42076871207, 0.424676209952], "type": "scatter"}], {"title": "F1 vs Precision", "xaxis": {"title": "Precision"}, "yaxis": {"title": "F1"}}, {"linkText": "Export to plot.ly", "showLink": true})</script><script type="text/javascript">window.removeEventListener("resize");window.addEventListener("resize", function(){Plotly.Plots.resize(document.getElementById("9336a8f3-b15a-40d7-bb9a-967ef9e12852"));});</script>		
	</body>
</html>
